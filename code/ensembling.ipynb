{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from previous investigations we found some models that fared well with the task, and hyperparameters enabling them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme()\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils import get_data, get_param_combinations\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"experiments_joined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(f\"../results/{filename}.csv\")\n",
    "results['classifier_params'] = results['classifier_params'].str.replace(\"'\", '\"')\n",
    "clf_params_df = results['classifier_params'].apply(json.loads)\n",
    "clf_params_df = pd.json_normalize(clf_params_df)\n",
    "results = pd.concat([results.drop(columns=['classifier_params']), clf_params_df], axis = 1)\n",
    "results = results[results['n_features'] < 10]\n",
    "results.drop(columns = ['random_state', 'n_jobs', 'probability', 'device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('accuracy_top_20pc').groupby(['n_features', 'classifier']).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in ensembling, we plan on using SVM, XGBoost and NN (and maybe RF if it proves viable to ensemble it with XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### what hyperparameters are best for SVM vs n_features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_df = results[results['classifier'] == 'SVC']\n",
    "SVM_best_df = SVM_df.sort_values('accuracy_top_20pc', ascending = False).groupby(['n_features']).first()\n",
    "SVM_best_df[['accuracy', 'accuracy_top_20pc', 'kernel', 'degree', 'gamma', 'coef0', 'probability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_df.sort_values('accuracy_top_20pc').groupby(['n_features','kernel','degree', 'gamma', 'coef0']).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM models by kernel vs n_features\n",
    "rbf seems to be the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.lineplot(\n",
    "    data=SVM_df,\n",
    "    x=\"n_features\",\n",
    "    y=\"accuracy_top_20pc\",\n",
    "    hue=\"kernel\",\n",
    "    markers=True,\n",
    ")\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in ensembling, we will try SVM with poly, rbf or sigmoid kernel, linear is not suited for the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what hyperparameters are best for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_df = results[results['classifier'] == 'XGBClassifier']\n",
    "XGB_best_df = XGB_df.sort_values('accuracy_top_20pc', ascending = False).groupby(['n_features']).first()\n",
    "XGB_best_df[['accuracy', 'accuracy_top_20pc','n_estimators', 'learning_rate','booster','min_child_weight', 'max_depth', 'tree_method']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what hyperparameters are best for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.classifier.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_df = results[results['classifier'] == 'MLPClassifier']\n",
    "NN_best_df = NN_df.sort_values('accuracy_top_20pc', ascending = False).groupby(['n_features']).first()\n",
    "NN_best_df[['accuracy', 'accuracy_top_20pc','hidden_layer_sizes', 'activation', 'solver', 'alpha', 'learning_rate_init', 'max_iter']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensembling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_function(proba, y_test, n_features):\n",
    "    proba_1 = np.array([proba[:,1], y_test]).T\n",
    "    proba_1 = proba_1[proba_1[:, 0].argsort()][::-1]\n",
    "    top_20pc = proba_1[: int(len(proba_1) * 0.2)]\n",
    "    score = sum(top_20pc[:,1]) * 10 * 1000/len(top_20pc) - n_features*200\n",
    "    return score\n",
    "\n",
    "\n",
    "def accuracy_top_20pc(proba, y_test):\n",
    "    proba_1 = np.array([proba[:,1], y_test]).T\n",
    "    proba_1 = proba_1[proba_1[:, 0].argsort()][::-1]\n",
    "    top_20pc = proba_1[: int(len(proba_1) * 0.2)]\n",
    "    acc_top_20pc = accuracy_score(top_20pc[:, 1], np.round(top_20pc[:, 0]))\n",
    "    return acc_top_20pc\n",
    "\n",
    "    \n",
    "\n",
    "class Ensemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, classifiers_params):\n",
    "        \"\"\"\n",
    "        Initialize the ensemble with classifiers and their parameters.\n",
    "        Parameters:\n",
    "        classifiers_params (list): List of tuples where each tuple is \n",
    "                                   (classifier_class, params_dict).\n",
    "        \"\"\"\n",
    "        self.classifiers_params = classifiers_params\n",
    "        self.classifiers = []\n",
    "        self.probas = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit all classifiers to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        X (array-like): Training features.\n",
    "        y (array-like): Training labels.\n",
    "        \"\"\"\n",
    "        self.classifiers = []\n",
    "        for clf_class, params in self.classifiers_params:\n",
    "            clf = clf_class(**params)\n",
    "            clf.fit(X, y)\n",
    "            self.classifiers.append(clf)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities using soft voting.\n",
    "\n",
    "        Parameters:\n",
    "        X (array-like): Test features.\n",
    "\n",
    "        Returns:\n",
    "        dict: Dictionary of classifier class names and predicted probabilities.\n",
    "        \"\"\"\n",
    "        self.probas = {clf.__class__.__name__: clf.predict_proba(X) for clf in self.classifiers}\n",
    "\n",
    "        return self.probas\n",
    "\n",
    "    def compare(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Compare all possible combinations of classifiers using a custom score function.\n",
    "\n",
    "        Parameters:\n",
    "        X (array-like): Test features.\n",
    "        y (array-like): Test labels.\n",
    "\n",
    "        Returns:\n",
    "        DataFrame: Results with combination and custom score.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        self.predict_proba(X_test)\n",
    "        classifier_names = list(self.probas.keys())\n",
    "        n_classifiers = len(classifier_names)\n",
    "\n",
    "        for r in range(1, n_classifiers + 1):\n",
    "            for subset in itertools.combinations(classifier_names, r):\n",
    "                probas = [self.probas[name] for name in subset]\n",
    "                avg_proba = np.mean(probas, axis=0)\n",
    "                score = score_function(avg_proba, y_test, X_test.shape[1])\n",
    "                results.append({\n",
    "                    'n_features': X_test.shape[1],\n",
    "                    'Combination': subset,\n",
    "                    'score': score,\n",
    "                    'accuracy' : accuracy_score(np.round(avg_proba[:,1]).astype(int), y_test),\n",
    "                    'accuracy_top_20pc' : accuracy_top_20pc(avg_proba, y_test)\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data()\n",
    "features_by_importance = [105,100,101,102,103,104,8,113,2,391] # top 10 features by importance from RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "for n_features in range(1, 7, 1):\n",
    "    X_features = X[:,features_by_importance[:n_features]]\n",
    "    SVC_params = {'kernel' : SVM_best_df.iloc[n_features-1].kernel,\n",
    "           'degree' : SVM_best_df.iloc[n_features-1].degree.astype(int),\n",
    "           'gamma' : SVM_best_df.iloc[n_features-1].gamma,\n",
    "           'coef0' : SVM_best_df.iloc[n_features-1].coef0,\n",
    "           'probability' : True\n",
    "           }\n",
    "    XGB_params = {'n_estimators' : XGB_best_df.iloc[n_features-1].n_estimators.astype(int),\n",
    "                     'learning_rate' : XGB_best_df.iloc[n_features-1].learning_rate,\n",
    "                     'booster' : XGB_best_df.iloc[n_features-1].booster,\n",
    "                     'min_child_weight' : XGB_best_df.iloc[n_features-1].min_child_weight.astype(int),\n",
    "                     'max_depth' : XGB_best_df.iloc[n_features-1].max_depth.astype(int),\n",
    "                     'tree_method' : XGB_best_df.iloc[n_features-1].tree_method\n",
    "                     }\n",
    "    MLP_params = {'hidden_layer_sizes' : NN_best_df.iloc[n_features-1].hidden_layer_sizes,\n",
    "                     'activation' : NN_best_df.iloc[n_features-1].activation,\n",
    "                     'solver' : NN_best_df.iloc[n_features-1].solver,\n",
    "                     'alpha' : NN_best_df.iloc[n_features-1].alpha,\n",
    "                     'learning_rate_init' : NN_best_df.iloc[n_features-1].learning_rate_init,\n",
    "                     'max_iter' : NN_best_df.iloc[n_features-1].max_iter.astype(int)}\n",
    "    classifier_params = [\n",
    "    (SVC, SVC_params),\n",
    "    (XGBClassifier, XGB_params),\n",
    "    (MLPClassifier, MLP_params)\n",
    "    ]\n",
    "    for r in range(42, 53, 1):\n",
    "       ensemble = Ensemble(classifier_params)\n",
    "       X_train, X_test, y_train, y_test = train_test_split(X_features, y, random_state=r, test_size=0.2)\n",
    "       ensemble.fit(X_train, y_train)\n",
    "       out = ensemble.compare(X_test, y_test)\n",
    "       results = pd.concat([results,out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('../results/ensemble_plot_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = results.groupby(['n_features', 'Combination']).agg(['mean', 'std'])\n",
    "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
    "grouped.to_csv('../results/ensemble_plot_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 features, grid search for SVM + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_params = {'kernel' : SVM_best_df.iloc[n_features-1].kernel,\n",
    "           'degree' : SVM_best_df.iloc[n_features-1].degree.astype(int),\n",
    "           'gamma' : SVM_best_df.iloc[n_features-1].gamma,\n",
    "           'coef0' : SVM_best_df.iloc[n_features-1].coef0,\n",
    "           'probability' : True\n",
    "           }\n",
    "XGB_params = {'n_estimators' : XGB_best_df.iloc[n_features-1].n_estimators.astype(int),\n",
    "                    'learning_rate' : XGB_best_df.iloc[n_features-1].learning_rate,\n",
    "                    'booster' : XGB_best_df.iloc[n_features-1].booster,\n",
    "                    'min_child_weight' : XGB_best_df.iloc[n_features-1].min_child_weight.astype(int),\n",
    "                    'max_depth' : XGB_best_df.iloc[n_features-1].max_depth.astype(int),\n",
    "                    'tree_method' : XGB_best_df.iloc[n_features-1].tree_method\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.DataFrame()\n",
    "classifiers = [\n",
    "    (\n",
    "        XGBClassifier,\n",
    "        {\n",
    "            \"n_estimators\": [150],\n",
    "            \"learning_rate\": [0.001, 0.01],\n",
    "            \"min_child_weight\" : [0, 1, 2, 3, 4],\n",
    "            \"subsample\" : [1/2],\n",
    "            \"lambda\" : [0.1, 0.5],\n",
    "            \"max_depth\" : [2, 6, 8],\n",
    "            \"tree_method\" : ['approx', 'hist']\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        SVC,\n",
    "        {\n",
    "        \"kernel\" : ['poly'],\n",
    "        \"degree\" : [3,4,5],\n",
    "        \"gamma\" : ['auto'],\n",
    "        \"coef0\" : [0.01, 0.2],\n",
    "        \"probability\" : [True]\n",
    "    }\n",
    "    )\n",
    "]\n",
    "XGB, XGB_params = classifiers[0]\n",
    "SVM, SVM_params = classifiers[1]\n",
    "XGB_param_combinations = get_param_combinations(XGB_params)\n",
    "SVM_param_combinations = get_param_combinations(SVM_params)\n",
    "for XGB_param_set in XGB_param_combinations:\n",
    "    for SVM_param_set in SVM_param_combinations:\n",
    "        for k in range(42,53,1):\n",
    "            ensemble = Ensemble([(XGB, XGB_param_set), (SVM, SVM_param_set)])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X[:,features_by_importance[:n_features]], y, test_size=0.2)\n",
    "            \n",
    "            start = time.time()\n",
    "            ensemble.fit(X_train, y_train)\n",
    "            out = ensemble.compare(X_test, y_test)\n",
    "            out['XGB_params'] = [XGB_param_set]*out.shape[0]\n",
    "            out['SVM_params'] = [SVM_param_set]*out.shape[0]\n",
    "            results = pd.concat([results, out])\n",
    "            results.to_csv('../results/ensemble_grid_search.csv', index = False)\n",
    "            print(out['score'])\n",
    "        print(f\"{start - time.time():.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('score', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top 3 models are run on more train/test splits for better evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../results/ensemble_grid_search.csv')\n",
    "grouped = df.groupby(['Combination', 'XGB_params', 'SVM_params']).agg(['mean', 'std'])\n",
    "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
    "top_3 = grouped.sort_values('score_mean', ascending=False).head(3)\n",
    "\n",
    "# Extract the XGB_params and SVM_params for the top 3 rows\n",
    "top_3_params = top_3.index.to_frame(index=False)[['XGB_params', 'SVM_params']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_param_sets = top_3_params['XGB_params']\n",
    "SVM_param_sets = top_3_params['SVM_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_params = [json.loads(i.replace(\"'\", '\"')) for i in XGB_param_sets]\n",
    "SVM_params = [json.loads(i.replace(\"'\", '\"').replace('True', '\"True\"')) for i in SVM_param_sets]\n",
    "for i in range(3):\n",
    "    SVM_params[i]['probability'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_4 = X[:,features_by_importance[:n_features]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "for XGB_param_set, SVM_param_set in zip(XGB_params, SVM_params):\n",
    "    for k in range(100):\n",
    "        ensemble = Ensemble([(XGBClassifier, XGB_param_set), (SVC, SVM_param_set)])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_4, y, test_size=0.2)\n",
    "        ensemble.fit(X_train, y_train)\n",
    "        out = ensemble.compare(X_test, y_test)\n",
    "        out['XGB_params'] = [XGB_param_set]*out.shape[0]\n",
    "        out['SVM_params'] = [SVM_param_set]*out.shape[0]\n",
    "        out['split_size'] = [0.2] * out.shape[0]\n",
    "        results = pd.concat([results, out])\n",
    "        results.to_csv('../results/final_ensembles.csv', index = False)\n",
    "        print(k, 'first')\n",
    "    for k in range(100):\n",
    "        ensemble = Ensemble([(XGBClassifier, XGB_param_set), (SVC, SVM_param_set)])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_4, y, test_size=0.35)\n",
    "        ensemble.fit(X_train, y_train)\n",
    "        out = ensemble.compare(X_test, y_test)\n",
    "        out['XGB_params'] = [XGB_param_set]*out.shape[0]\n",
    "        out['SVM_params'] = [SVM_param_set]*out.shape[0]\n",
    "        out['split_size'] = [0.35] * out.shape[0]\n",
    "        results = pd.concat([results, out])\n",
    "        results.to_csv('../results/final_ensembles.csv', index = False)\n",
    "        print(k, 'second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_by_importance = [105,100,101,102,103,104,8,113,2,391]\n",
    "X_4 = X[:,features_by_importance[:n_features]]\n",
    "results = pd.DataFrame()\n",
    "for XGB_param_set, SVM_param_set in zip(XGB_params, SVM_params):\n",
    "    for k in range(100):\n",
    "        ensemble = Ensemble([(XGBClassifier, XGB_param_set), (SVC, SVM_param_set)])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_4, y, test_size=0.2)\n",
    "        ensemble.fit(X_train, y_train)\n",
    "        out = ensemble.compare(X_test, y_test)\n",
    "        out['XGB_params'] = [XGB_param_set]*out.shape[0]\n",
    "        out['SVM_params'] = [SVM_param_set]*out.shape[0]\n",
    "        out['split_size'] = [0.2] * out.shape[0]\n",
    "        results = pd.concat([results, out])\n",
    "        results.to_csv('../results/final_ensembles_orig_features.csv', index = False)\n",
    "        print(k, 'first')\n",
    "    for k in range(100):\n",
    "        ensemble = Ensemble([(XGBClassifier, XGB_param_set), (SVC, SVM_param_set)])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_4, y, test_size=0.35)\n",
    "        ensemble.fit(X_train, y_train)\n",
    "        out = ensemble.compare(X_test, y_test)\n",
    "        out['XGB_params'] = [XGB_param_set]*out.shape[0]\n",
    "        out['SVM_params'] = [SVM_param_set]*out.shape[0]\n",
    "        out['split_size'] = [0.35] * out.shape[0]\n",
    "        results = pd.concat([results, out])\n",
    "        results.to_csv('../results/final_ensembles_orig_features.csv', index = False)\n",
    "        print(k, 'second')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
